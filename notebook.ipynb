{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/VahidZee/DRMADE.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1\n",
    "! unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd DRMADE/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch as t\n",
    "from torch.optim import lr_scheduler, Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from src.utils.data import DatasetSelection\n",
    "from src.model.model import DRMADE\n",
    "import src.config as config\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set_output_dirs('./output')\n",
    "\n",
    "# variables\n",
    "batch_size = 512\n",
    "test_batch_size = 512\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config.normal_classes = [8]\n",
    "\n",
    "hidden_layers = [64, 64, 64]\n",
    "num_masks = 5\n",
    "num_mix = 5\n",
    "latent_size = 32\n",
    "\n",
    "latent_cor_regularization_factor = 0.01\n",
    "latent_zero_regularization_factor = 0.01\n",
    "\n",
    "noise_factor = 0\n",
    "\n",
    "config.parameters_regularization_factor = [0, 0.01]\n",
    "\n",
    "base_lr = 0.0002\n",
    "lr_decay = config.lr_decay\n",
    "lr_half_schedule = 512\n",
    "\n",
    "# accessory functions\n",
    "noise_function = lambda x: noise_factor * (\n",
    "        2 * t.FloatTensor(*x).to(device).uniform_() - 1)  # (x will be the input shape tuple)\n",
    "lr_multiplicative_factor_lambda = lambda epoch: 0.5 if (epoch + 1) % lr_half_schedule == 0 else lr_decay\n",
    "\n",
    "# reproducibility\n",
    "t.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "\n",
    "# type initialization\n",
    "t.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loading training data')\n",
    "train_data = DatasetSelection(datasets.MNIST, classes=config.normal_classes, train=True)\n",
    "print('loading validation data')\n",
    "validation_data = DatasetSelection(datasets.MNIST, classes=config.normal_classes, train=False)\n",
    "print('loading test data')\n",
    "test_data = DatasetSelection(datasets.MNIST, train=False)\n",
    "\n",
    "input_shape = train_data.input_shape()\n",
    "\n",
    "print('initializing data loaders')\n",
    "train_loader = train_data.get_dataloader(shuffle=True, batch_size=batch_size)\n",
    "validation_loader = validation_data.get_dataloader(shuffle=False, batch_size=batch_size)\n",
    "test_loader = test_data.get_dataloader(shuffle=False,batch_size=test_batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('initializing model')\n",
    "model = DRMADE(input_shape[0], latent_size, hidden_layers, num_masks=num_masks, num_mix=num_mix).to(device)\n",
    "model.encoder = model.encoder.to(device)\n",
    "model.made = model.made.to(device)\n",
    "\n",
    "# setting up tensorboard data summerizer\n",
    "model_name = '{}-rl_cor={}{}-rl_zero={}eps{}-nz={}-Adam,lr={},dc={},s={}'.format(\n",
    "    model.name, \n",
    "    \"abs\" if config.latent_cor_regularization_abs else \"noabs\",\n",
    "    latent_cor_regularization_factor,\n",
    "    latent_zero_regularization_factor,\n",
    "    config.latent_zero_regularization_eps,\n",
    "    noise_factor,\n",
    "    base_lr,\n",
    "    lr_decay,\n",
    "    lr_half_schedule\n",
    ")\n",
    "writer = SummaryWriter(\n",
    "    log_dir=config.runs_dir + f'/{model_name}')\n",
    "\n",
    "print('initializing optimizer')\n",
    "optimizer = Adam(model.parameters(), lr=base_lr)\n",
    "print('initializing learning rate scheduler')\n",
    "scheduler = lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lr_multiplicative_factor_lambda,\n",
    "                                          last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    data = {\n",
    "        'loss': 0.0,\n",
    "        'log_prob': 0.0,\n",
    "        'latent_regularization/correlation': 0.0,\n",
    "        'latent_regularization/zero': 0.0,\n",
    "        'parameters_regularization': [0.0 for i in range(config.num_dist_parameters)]\n",
    "    }\n",
    "\n",
    "    time_ = time.time()\n",
    "    for batch_idx, (images, _) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        if noise_factor:\n",
    "            noisy_images = images + noise_function(images.shape)\n",
    "            noisy_images.clamp_(min=-1, max=1)\n",
    "            noised_features = model.encoder(noisy_images)\n",
    "        features = model.encoder(images)\n",
    "\n",
    "        log_prob = 0.0\n",
    "        parameters_regularization = [0.0 for i in range(config.num_dist_parameters)]\n",
    "        for i in range(num_masks):\n",
    "            model.made.update_masks()\n",
    "            if noise_factor:\n",
    "                noised_output = model.made(noised_features)\n",
    "                parameters = model.get_dist_parameters(noised_output)\n",
    "                log_prob += model.log_prob(features, noised_output, parameters=parameters)\n",
    "            else:\n",
    "                output = model.made(features)\n",
    "                parameters = model.get_dist_parameters(output)\n",
    "                log_prob += model.log_prob(features, output=output, parameters=parameters)\n",
    "            for j, regularization in enumerate(config.parameters_regularization):\n",
    "                parameters_regularization[j] += regularization(parameters[j])\n",
    "\n",
    "        log_prob /= num_masks\n",
    "        for i in range(config.num_dist_parameters):\n",
    "            parameters_regularization[i] /= num_masks\n",
    "\n",
    "        latent_cor_regularization = model.latent_cor_regularization(\n",
    "            noised_features) if noise_factor else model.latent_cor_regularization(features)\n",
    "\n",
    "        latent_zero_regularization = model.latent_zero_regularization(\n",
    "            noised_features) if noise_factor else model.latent_zero_regularization(features)\n",
    "\n",
    "        loss = -log_prob + latent_cor_regularization_factor * latent_cor_regularization + latent_zero_regularization_factor * latent_zero_regularization\n",
    "        for i, factor in enumerate(config.parameters_regularization_factor):\n",
    "            loss += factor * parameters_regularization[i]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        data['loss'] += loss / batch_size\n",
    "        data['log_prob'] += log_prob / batch_size\n",
    "        data['latent_regularization/correlation'] += latent_cor_regularization / batch_size\n",
    "        data['latent_regularization/zero'] += latent_zero_regularization / batch_size\n",
    "        for i, reg in enumerate(parameters_regularization):\n",
    "            data['parameters_regularization'][i] += reg / num_masks\n",
    "\n",
    "        if config.log_train_loop_interval and (batch_idx + 1) % config.log_train_loop_interval == 0:\n",
    "            print(\n",
    "                '\\t{:3d}/{:3d} - loss : {:.4f}, time : {:.3f}s'.format(\n",
    "                    batch_idx, len(train_loader), data['loss'] / (1 + batch_idx), time.time() - time_)\n",
    "            )\n",
    "            time_ = time.time()\n",
    "        for key in data:\n",
    "            if isinstance(data[key], list):\n",
    "                for i in range(len(data[key])):\n",
    "                    data[key][i] /= len(train_loader)\n",
    "            else:\n",
    "                data[key] /= len(train_loader)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop():\n",
    "    data = {\n",
    "        'loss': 0.0,\n",
    "        'log_prob': 0.0,\n",
    "        'latent_regularization/correlation': 0.0,\n",
    "        'latent_regularization/zero': 0.0,\n",
    "        'parameters_regularization': [0.0 for i in range(config.num_dist_parameters)]\n",
    "    }\n",
    "\n",
    "    time_ = time.time()\n",
    "    with t.no_grad():\n",
    "        for batch_idx, (images, _) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            if noise_factor:\n",
    "                noisy_images = images + noise_function(images.shape)\n",
    "                noisy_images.clamp_(min=-1, max=1)\n",
    "                noised_features = model.encoder(noisy_images)\n",
    "            features = model.encoder(images)\n",
    "\n",
    "            log_prob = 0.0\n",
    "            parameters_regularization = [0.0 for i in range(config.num_dist_parameters)]\n",
    "            for i in range(num_masks):\n",
    "                model.made.update_masks()\n",
    "                output = model.made(features)\n",
    "                parameters = model.get_dist_parameters(output)\n",
    "                log_prob += model.log_prob(features, output=output, parameters=parameters)\n",
    "                for j, regularization in enumerate(config.parameters_regularization):\n",
    "                    parameters_regularization[j] += regularization(parameters[j])\n",
    "\n",
    "            log_prob /= num_masks\n",
    "            for i in range(config.num_dist_parameters):\n",
    "                parameters_regularization[i] /= num_masks\n",
    "\n",
    "            latent_cor_regularization = model.latent_cor_regularization(\n",
    "                noised_features) if noise_factor else model.latent_cor_regularization(features)\n",
    "\n",
    "            latent_zero_regularization = model.latent_zero_regularization(\n",
    "                noised_features) if noise_factor else model.latent_zero_regularization(features)\n",
    "\n",
    "            loss = -log_prob + latent_cor_regularization_factor * latent_cor_regularization + \\\n",
    "                   latent_zero_regularization_factor * latent_zero_regularization\n",
    "            for i, factor in enumerate(config.parameters_regularization_factor):\n",
    "                loss += factor * parameters_regularization[i]\n",
    "\n",
    "            data['loss'] += loss / batch_size\n",
    "            data['log_prob'] += log_prob / batch_size\n",
    "            data['latent_regularization/correlation'] += latent_cor_regularization / batch_size\n",
    "            data['latent_regularization/zero'] += latent_zero_regularization / batch_size\n",
    "            for i, reg in enumerate(parameters_regularization):\n",
    "                data['parameters_regularization'][i] += reg / num_masks\n",
    "\n",
    "            if config.log_validation_loop_interval and (batch_idx + 1) % config.log_validation_loop_interval == 0:\n",
    "                print(\n",
    "                    '\\t{:3d}/{:3d} - loss : {:.4f}, time : {:.3f}s'.format(\n",
    "                        batch_idx, len(train_loader), data['loss'] / (1 + batch_idx), time.time() - time_)\n",
    "                )\n",
    "                time_ = time.time()\n",
    "        for key in data:\n",
    "            if isinstance(data[key], list):\n",
    "                for i in range(len(data[key])):\n",
    "                    data[key][i] /= len(train_loader)\n",
    "            else:\n",
    "                data[key] /= len(train_loader)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loop(data_loader, record_input_images=False):\n",
    "    with t.no_grad():\n",
    "        scores = t.Tensor().to(device)\n",
    "        features = t.Tensor().to(device)\n",
    "        labels = np.empty(0, dtype=np.int8)\n",
    "        input_images = t.Tensor().to(device)\n",
    "        time_ = time.time()\n",
    "        for batch_idx, (images, label) in enumerate(data_loader):\n",
    "            images = images.to(device)\n",
    "            if record_input_images:\n",
    "                input_images = t.cat((input_images, images), dim=0)\n",
    "\n",
    "            output, latent = model(images)\n",
    "            scores = t.cat((scores, model.log_prob_hitmap(latent, output).sum(1)), dim=0)\n",
    "            features = t.cat((features, latent), dim=0)\n",
    "            labels = np.append(labels, label.numpy().astype(np.int8), axis=0)\n",
    "            if config.log_evaluation_loop_interval and (batch_idx + 1) % config.log_evaluation_loop_interval == 0:\n",
    "                print(\n",
    "                    '\\t{:3d}/{:3d} - time : {:.3f}s'.format(\n",
    "                        batch_idx, len(data_loader), time.time() - time_)\n",
    "                )\n",
    "                time_ = time.time()\n",
    "    return scores, features, labels, input_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def submit_loop_data(data, title, epoch):\n",
    "    for key in data:\n",
    "        if isinstance(data[key], list):\n",
    "            for i in range(len(data[key])):\n",
    "                writer.add_scalar(f'{key}/{title}/{i}', data[key][i], epoch)\n",
    "        else:\n",
    "            writer.add_scalar(f'{key}/{title}', data[key], epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_encoder_weights(epoch):\n",
    "    writer.add_histogram('encoder/conv1',model.encoder.conv1.weight, epoch)\n",
    "    writer.add_histogram('encoder/conv2',model.encoder.conv2.weight, epoch)\n",
    "    writer.add_histogram('encoder/conv3',model.encoder.conv3.weight, epoch)\n",
    "    writer.add_histogram('encoder/fc1',model.encoder.fc1.weight, epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    for epoch in range(5121):\n",
    "        print('epoch {:4} - lr: {}'.format(epoch, optimizer.param_groups[0][\"lr\"]))\n",
    "        if config.validation_interval and epoch % config.validation_interval == 0:\n",
    "            validation_results = validation_loop()\n",
    "            submit_loop_data(validation_results, 'validation', epoch)\n",
    "\n",
    "        if config.evaluation_interval and epoch % config.evaluation_interval == 0:\n",
    "            scores, features, labels, _ = evaluate_loop(test_loader)\n",
    "            writer.add_scalar('auc', roc_auc_score(y_true=np.isin(labels, config.normal_classes).astype(np.int8),\n",
    "                                                   y_score=scores.cpu()), epoch)\n",
    "            writer.add_histogram('log_probs/anomaly', scores[(np.isin(labels, config.normal_classes) == False)], epoch)\n",
    "            scores, features, labels, _ = evaluate_loop(train_loader)\n",
    "            writer.add_histogram('log_probs/normal', scores, epoch)\n",
    "            writer.flush()\n",
    "\n",
    "        if config.embedding_interval and (epoch + 1) % config.embedding_interval == 0:\n",
    "            scores, features, labels, input_images = evaluate_loop(test_loader, record_input_images=True)\n",
    "            writer.add_embedding(features, metadata=labels, label_img=input_images, global_step=epoch,\n",
    "                                 tag=f'{model_name}/test')\n",
    "            writer.flush()\n",
    "        \n",
    "        if config.track_weights_interval and epoch % config.track_weights_interval == 0:\n",
    "            submit_encoder_weights(epoch)\n",
    "        \n",
    "        train_results = train_loop()\n",
    "        submit_loop_data(train_results, 'train', epoch)\n",
    "\n",
    "        writer.add_scalar('learning_rate', optimizer.param_groups[0][\"lr\"], epoch)\n",
    "        scheduler.step()\n",
    "\n",
    "        if config.save_interval and (epoch + 1) % config.save_interval == 0:\n",
    "            model.save(config.models_dir + f'/{model_name}-E{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir \"{}\" --host 0.0.0.0 --port 6006 &'.format(config.runs_dir)\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().system_raw('./ngrok http 6006 &')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir f\"{config.runs_dir}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
