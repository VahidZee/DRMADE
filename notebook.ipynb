{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/VahidZee/DRMADE.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd DRMADE/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch as t\n",
    "from torch.optim import lr_scheduler, Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from src.utils.data import DatasetSelection\n",
    "from src.model.model import DRMADE\n",
    "import src.config as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "batch_size = 32\n",
    "test_batch_size = 32\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config.normal_classes = [8]\n",
    "\n",
    "hidden_layers = [64, 64, 64]\n",
    "num_masks = 2\n",
    "num_mix = 5\n",
    "latent_size = 16\n",
    "\n",
    "latent_regularization_factor = 0.5\n",
    "noise_factor = 0\n",
    "\n",
    "base_lr = 0.0002\n",
    "lr_decay = config.lr_decay\n",
    "lr_half_schedule = 256\n",
    "\n",
    "# accessory functions\n",
    "noise_function = lambda x: noise_factor * (\n",
    "        2 * t.FloatTensor(*x).to(device).uniform_() - 1)  # (x will be the input shape tuple)\n",
    "lr_multiplicative_factor_lambda = lambda epoch: 0.5 if (epoch + 1) % lr_half_schedule == 0 else lr_decay\n",
    "\n",
    "# reproducibility\n",
    "t.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "\n",
    "# type initialization\n",
    "t.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loading training data')\n",
    "train_data = DatasetSelection(datasets.MNIST, classes=config.normal_classes, train=True)\n",
    "print('loading validation data')\n",
    "validation_data = DatasetSelection(datasets.MNIST, classes=config.normal_classes, train=False)\n",
    "print('loading test data')\n",
    "test_data = DatasetSelection(datasets.MNIST, train=False)\n",
    "\n",
    "input_shape = train_data.input_shape()\n",
    "\n",
    "print('initializing data loaders')\n",
    "train_loader = train_data.get_dataloader(shuffle=True)\n",
    "validation_loader = validation_data.get_dataloader(shuffle=False)\n",
    "test_loader = test_data.get_dataloader(shuffle=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('initializing model')\n",
    "model = DRMADE(input_shape[0], latent_size, hidden_layers, num_masks=num_masks, num_mix=num_mix)\n",
    "\n",
    "# setting up tensorboard data summerizer\n",
    "model_name = f'{model.name}-rl={latent_regularization_factor}-nz={noise_factor}-Adam,lr={base_lr},dc={lr_decay},s={lr_half_schedule}'\n",
    "writer = SummaryWriter(\n",
    "    log_dir=config.runs_dir + f'/{model_name}')\n",
    "\n",
    "print('initializing optimizer')\n",
    "optimizer = Adam(model.parameters(), lr=base_lr)\n",
    "print('initializing learning rate scheduler')\n",
    "scheduler = lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lr_multiplicative_factor_lambda, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    data = {\n",
    "        'loss': 0.0,\n",
    "        'log_prob': 0.0,\n",
    "        'latent_regularization': 0.0,\n",
    "        'parameters_regularization': [0.0 for i in range(config.num_dist_parameters)]\n",
    "    }\n",
    "\n",
    "    time_ = time.time()\n",
    "    for batch_idx, (images, _) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        if noise_factor:\n",
    "            noisy_images = images + noise_function(images.shape)\n",
    "            noisy_images.clamp_(min=-1, max=1)\n",
    "            noised_features = model.encoder(noisy_images)\n",
    "        features = model.encoder(images)\n",
    "\n",
    "        log_prob = 0.0\n",
    "        parameters_regularization = [0.0 for i in range(config.num_dist_parameters)]\n",
    "        for i in range(num_masks):\n",
    "            model.made.update_masks()\n",
    "            if noise_factor:\n",
    "                noised_output = model.made(noised_features)\n",
    "                parameters = model.get_dist_parameters(noised_output)\n",
    "                log_prob += model.log_prob(features, noised_output, parameters=parameters)\n",
    "            else:\n",
    "                output = model.made(features)\n",
    "                parameters = model.get_dist_parameters(output)\n",
    "                log_prob += model.log_prob(features, output=output, parameters=parameters)\n",
    "            for j, regularization in enumerate(config.parameters_regularization):\n",
    "                parameters_regularization[j] += regularization(parameters[j])\n",
    "\n",
    "        log_prob /= num_masks\n",
    "        for i in range(config.num_dist_parameters):\n",
    "            parameters_regularization[i] /= num_masks\n",
    "\n",
    "        latent_regularization = model.latent_regularization_term(\n",
    "            noised_features) if noise_factor else model.latent_regularization_term(features)\n",
    "\n",
    "        loss = log_prob + latent_regularization_factor * latent_regularization\n",
    "        for i, factor in enumerate(config.parameters_regularization_factor):\n",
    "            loss += factor * parameters_regularization[i]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        data['loss'] += loss / batch_size\n",
    "        data['log_prob'] += log_prob / batch_size\n",
    "        data['latent_regularization'] += latent_regularization / batch_size\n",
    "        for i in parameters_regularization:\n",
    "            data['parameters_regularization'] += i / num_masks\n",
    "\n",
    "        if config.log_train_loop_interval and (batch_idx + 1) % config.log_train_loop_interval == 0:\n",
    "            print(\n",
    "                '\\t{:3d}/{:3d} - loss : {:.4f}, time : {:.3f}s'.format(\n",
    "                    batch_idx, len(train_loader), data['loss'] / (1 + batch_idx), time.time() - time_)\n",
    "            )\n",
    "            time_ = time.time()\n",
    "        for key in data:\n",
    "            if isinstance(data[key], list):\n",
    "                for i in range(len(data[key])):\n",
    "                    data[key][i] /= len(train_loader)\n",
    "            else:\n",
    "                data[key] /= len(train_loader)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop():\n",
    "    data = {\n",
    "        'loss': 0.0,\n",
    "        'log_prob': 0.0,\n",
    "        'latent_regularization': 0.0,\n",
    "        'parameters_regularization': [0.0 for i in range(config.num_dist_parameters)]\n",
    "    }\n",
    "\n",
    "    time_ = time.time()\n",
    "    with t.no_grad():\n",
    "        for batch_idx, (images, _) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            if noise_factor:\n",
    "                noisy_images = images + noise_function(images.shape)\n",
    "                noisy_images.clamp_(min=-1, max=1)\n",
    "                noised_features = model.encoder(noisy_images)\n",
    "            features = model.encoder(images)\n",
    "\n",
    "            log_prob = 0.0\n",
    "            parameters_regularization = [0.0 for i in range(config.num_dist_parameters)]\n",
    "            for i in range(num_masks):\n",
    "                model.made.update_masks()\n",
    "                output = model.made(features)\n",
    "                parameters = model.get_dist_parameters(output)\n",
    "                log_prob += model.log_prob(features, output=output, parameters=parameters)\n",
    "                for j, regularization in enumerate(config.parameters_regularization):\n",
    "                    parameters_regularization[j] += regularization(parameters[j])\n",
    "\n",
    "            log_prob /= num_masks\n",
    "            for i in range(config.num_dist_parameters):\n",
    "                parameters_regularization[i] /= num_masks\n",
    "\n",
    "            latent_regularization = model.latent_regularization_term(\n",
    "                noised_features) if noise_factor else model.latent_regularization_term(features)\n",
    "\n",
    "            loss = log_prob + latent_regularization_factor * latent_regularization\n",
    "            for i, factor in enumerate(config.parameters_regularization_factor):\n",
    "                loss += factor * parameters_regularization[i]\n",
    "\n",
    "            data['loss'] += loss / batch_size\n",
    "            data['log_prob'] += log_prob / batch_size\n",
    "            data['latent_regularization'] += latent_regularization / batch_size\n",
    "            for i in parameters_regularization:\n",
    "                data['parameters_regularization'] += i / num_masks\n",
    "\n",
    "            if config.log_validation_loop_interval and (batch_idx + 1) % config.log_validation_loop_interval == 0:\n",
    "                print(\n",
    "                    '\\t{:3d}/{:3d} - loss : {:.4f}, time : {:.3f}s'.format(\n",
    "                        batch_idx, len(train_loader), data['loss'] / (1 + batch_idx), time.time() - time_)\n",
    "                )\n",
    "                time_ = time.time()\n",
    "        for key in data:\n",
    "            if isinstance(data[key], list):\n",
    "                for i in range(len(data[key])):\n",
    "                    data[key][i] /= len(train_loader)\n",
    "            else:\n",
    "                data[key] /= len(train_loader)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loop(data_loader):\n",
    "    with t.no_grad():\n",
    "        scores = t.Tensor().to(device)\n",
    "        features = t.Tensor().to(device)\n",
    "        labels = np.empty(0, dtype=np.int8)\n",
    "        time_ = time.time()\n",
    "        for batch_idx, (images, label) in enumerate(data_loader):\n",
    "            images = images.to(device)\n",
    "            output, latent = model(images)\n",
    "            scores = t.cat((scores, model.log_prob_hitmap(latent, output).sum(1)), dim=0)\n",
    "            features = t.cat((features, latent), dim=0)\n",
    "            labels = np.append(labels, label.numpy().astype(np.int8), axis=0)\n",
    "            if config.log_evaluation_loop_interval and (batch_idx + 1) % config.log_evaluation_loop_interval == 0:\n",
    "                print(\n",
    "                    '\\t{:3d}/{:3d} - time : {:.3f}s'.format(\n",
    "                        batch_idx, len(data_loader), time.time() - time_)\n",
    "                )\n",
    "                time_ = time.time()\n",
    "    return scores, features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def submit_loop_data(data, title, epoch):\n",
    "    for key in data:\n",
    "        if isinstance(data[key], list):\n",
    "            for i in range(len(data[key])):\n",
    "                writer.add_scalar(f'{key}/{title}/{i}', data[key][i], epoch)\n",
    "        else:\n",
    "            writer.add_scalar(f'{key}/{title}', data[key], epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train():\n",
    "    for epoch in range(config.max_epoch):\n",
    "        print('epoch {:4} - lr: {}'.format(epoch, optimizer.param_groups[0][\"lr\"]))\n",
    "        validation_loss = validation_loop(epoch)\n",
    "        scores, is_pos = evaluate_loop(epoch)\n",
    "        train_loss = train_loop(epoch)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(train_loss,validation_loss, roc_auc_score(y_true=is_pos.astype(np.int8), y_score=scores.cpu()))\n",
    "        writer.add_scalar('auc', roc_auc_score(y_true=is_pos.astype(np.int8), y_score=scores.cpu()), epoch)\n",
    "        writer.add_scalar('loss/validation', validation_loss, epoch)\n",
    "        writer.add_scalar('loss/training', train_loss, epoch)\n",
    "        writer.add_scalar('learning-rate', optimizer.param_groups[0][\"lr\"], epoch)\n",
    "        writer.add_histogram('test/positive-scores', scores[is_pos], epoch)\n",
    "        writer.add_histogram('test/negative-scores', scores[(is_pos == False)], epoch)\n",
    "        \n",
    "        writer.flush()\n",
    "        if config.save_interval and (epoch + 1) % config.save_interval == 0:\n",
    "            model.save(config.models_dir + f'/{model_name}-E{epoch}.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for epoch in range(config.max_epoch):\n",
    "        print('epoch {:4} - lr: {}'.format(epoch, optimizer.param_groups[0][\"lr\"]))\n",
    "        if config.validation_interval and (epoch + 1) % config.validation_interval == 0:\n",
    "            validation_results = validation_loop()\n",
    "            submit_loop_data(validation_results, 'validation', epoch)\n",
    "        if config.evaluation_interval and (epoch + 1) % config.evaluation_interval == 0:\n",
    "            scores, features, labels = evaluate_loop(test_loader)\n",
    "            writer.add_scalar('auc', roc_auc_score(y_true=np.isin(labels, config.normal_classes).astype(np.int8),\n",
    "                                                   y_score=scores.cpu()), epoch)\n",
    "\n",
    "        train_results = train_loop()\n",
    "        submit_loop_data(train_results, 'train', epoch)\n",
    "        scheduler.step()\n",
    "\n",
    "        writer.flush()\n",
    "        if config.save_interval and (epoch + 1) % config.save_interval == 0:\n",
    "            model.save(config.models_dir + f'/{model_name}-E{epoch}.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}