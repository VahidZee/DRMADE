{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/VahidZee/DRMADE.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1\n",
    "! unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd DRMADE/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch as t\n",
    "from torch.optim import lr_scheduler, Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from src.utils.data import DatasetSelection\n",
    "from src.model.model import DRMADE\n",
    "import src.config as config\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set_output_dirs('./output')\n",
    "\n",
    "# variables\n",
    "train_batch_size = 512\n",
    "validation_batch_size = 128\n",
    "test_batch_size = 512\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "config.normal_classes = [8]\n",
    "\n",
    "hidden_layers = [64, 64, 64]\n",
    "num_masks = 5\n",
    "num_mix = 5\n",
    "latent_size = 8\n",
    "\n",
    "latent_cor_regularization_factor = 0\n",
    "latent_zero_regularization_factor = 0\n",
    "latent_variance_regularization_factor = 0\n",
    "latent_distance_regularization_factor = 0\n",
    "distance_factor = 1\n",
    "\n",
    "noise_factor = 0.2\n",
    "\n",
    "config.parameters_regularization_factor = [0, 0.001]\n",
    "\n",
    "base_lr = 0.0002\n",
    "lr_decay = config.lr_decay\n",
    "lr_half_schedule = 512\n",
    "\n",
    "# accessory functions\n",
    "noise_function = lambda x: noise_factor * (\n",
    "        2 * t.FloatTensor(*x).to(device).uniform_() - 1)  # (x will be the input shape tuple)\n",
    "lr_multiplicative_factor_lambda = lambda epoch: 0.5 if (epoch + 1) % lr_half_schedule == 0 else lr_decay\n",
    "\n",
    "# reproducibility\n",
    "t.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "\n",
    "# type initialization\n",
    "t.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loading training data')\n",
    "train_data = DatasetSelection(datasets.MNIST, classes=config.normal_classes, train=True)\n",
    "print('loading validation data')\n",
    "validation_data = DatasetSelection(datasets.MNIST, classes=config.normal_classes, train=False)\n",
    "print('loading test data')\n",
    "test_data = DatasetSelection(datasets.MNIST, train=False)\n",
    "\n",
    "input_shape = train_data.input_shape()\n",
    "\n",
    "print('initializing data loaders')\n",
    "train_loader = train_data.get_dataloader(shuffle=True, batch_size=train_batch_size)\n",
    "validation_loader = validation_data.get_dataloader(shuffle=False, batch_size=validation_batch_size)\n",
    "test_loader = test_data.get_dataloader(shuffle=False,batch_size=test_batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('initializing model')\n",
    "model = DRMADE(input_shape[1], input_shape[0], latent_size, hidden_layers, num_masks=num_masks, num_mix=num_mix).to(device)\n",
    "model.encoder = model.encoder.to(device)\n",
    "model.made = model.made.to(device)\n",
    "model.decoder = model.decoder.to(device)\n",
    "\n",
    "# setting up tensorboard data summerizer\n",
    "model_name = '{}{}{}{}{}{}|distance{}|Adam,lr{},dc{},s{}'.format(\n",
    "    model.name,\n",
    "    '|rl_correlation{}{}'.format(\n",
    "        \"abs\" if config.latent_cor_regularization_abs else \"noabs\",\n",
    "        latent_cor_regularization_factor,\n",
    "    ) if latent_cor_regularization_factor else '',\n",
    "    '|rl_variance{}'.format(\n",
    "        latent_variance_regularization_factor,\n",
    "    ) if latent_variance_regularization_factor else '',\n",
    "    '|rl_zero{}eps{}'.format(\n",
    "        latent_zero_regularization_factor,\n",
    "        config.latent_zero_regularization_eps,\n",
    "    ) if latent_zero_regularization_factor else '',\n",
    "    '|rl_distance{}{},norm{}'.format(\n",
    "        'normalized' if config.latent_distance_normalize_features else '',\n",
    "        latent_distance_regularization_factor,\n",
    "        config.latent_distance_norm,\n",
    "    ) if latent_distance_regularization_factor else '',\n",
    "    '|nz{}'.format(noise_factor) if noise_factor else '',\n",
    "    distance_factor,\n",
    "    base_lr,\n",
    "    lr_decay,\n",
    "    lr_half_schedule\n",
    ")\n",
    "print(model_name)\n",
    "Path(config.models_dir + f'/{model_name}').mkdir(parents=True, exist_ok=True)\n",
    "writer = SummaryWriter(\n",
    "    log_dir=config.runs_dir + f'/{model_name}')\n",
    "\n",
    "print('initializing optimizer')\n",
    "optimizer = Adam(model.parameters(), lr=base_lr)\n",
    "print('initializing learning rate scheduler')\n",
    "scheduler = lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lr_multiplicative_factor_lambda,\n",
    "                                          last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_feed_loop(data_loader, optimize=True, log_interval=config.log_data_feed_loop_interval, loop_name=''):\n",
    "    data = {\n",
    "        'loss': 0.0,\n",
    "        'log_prob': 0.0,\n",
    "        'decoder_distance':0.0,\n",
    "        'latent_regularization/correlation': 0.0,\n",
    "        'latent_regularization/zero': 0.0,\n",
    "        'latent_regularization/variance': 0.0,\n",
    "        'latent_regularization/distance': 0.0,\n",
    "        'parameters_regularization': [0.0 for i in range(config.num_dist_parameters)]\n",
    "    }\n",
    "\n",
    "    time_ = time.time()\n",
    "    with t.set_grad_enabled(optimize):\n",
    "        for batch_idx, (images, _) in enumerate(data_loader):\n",
    "            images = images.to(device)\n",
    "            if noise_factor:\n",
    "                noisy_images = images + noise_function(images.shape)\n",
    "                noisy_images.clamp_(min=-1, max=1)\n",
    "                noised_features = model.encoder(noisy_images)\n",
    "            features = model.encoder(images)\n",
    "\n",
    "            log_prob = 0.0\n",
    "            parameters_regularization = [0.0 for i in range(config.num_dist_parameters)]\n",
    "            for i in range(num_masks):\n",
    "                model.made.update_masks()\n",
    "                if noise_factor:\n",
    "                    noised_output = model.made(noised_features)\n",
    "                    parameters = model.made.get_dist_parameters(noised_output)\n",
    "                    log_prob += model.made.log_prob(features, noised_output, parameters=parameters)\n",
    "                else:\n",
    "                    output = model.made(features)\n",
    "                    parameters = model.made.get_dist_parameters(output)\n",
    "                    log_prob += model.made.log_prob(features, output=output, parameters=parameters)\n",
    "                for j, regularization in enumerate(config.parameters_regularization):\n",
    "                    parameters_regularization[j] += regularization(parameters[j])\n",
    "\n",
    "            log_prob /= num_masks\n",
    "\n",
    "            for i in range(config.num_dist_parameters):\n",
    "                parameters_regularization[i] /= num_masks\n",
    "\n",
    "            latent_cor_regularization = model.encoder.latent_cor_regularization(\n",
    "                noised_features) if noise_factor else model.encoder.latent_cor_regularization(features)\n",
    "            latent_var_regularization = model.encoder.latent_var_regularization(\n",
    "                noised_features) if noise_factor else model.encoder.latent_var_regularization(features)\n",
    "            latent_zero_regularization = model.encoder.latent_zero_regularization(\n",
    "                noised_features) if noise_factor else model.encoder.latent_zero_regularization(features)\n",
    "            latent_distance_regularization = model.encoder.latent_distance_regularization(\n",
    "                noised_features) if noise_factor else model.encoder.latent_distance_regularization(features)\n",
    "            \n",
    "            reconstructed_image = model.decoder(noised_features) if noise_factor else model.decoder(features)\n",
    "            decoder_distance = model.decoder.distance(images, reconstructed_image).sum()\n",
    "            \n",
    "            loss = -log_prob + distance_factor * decoder_distance\n",
    "            if latent_cor_regularization_factor:\n",
    "                loss += latent_cor_regularization_factor * latent_cor_regularization\n",
    "            if latent_zero_regularization_factor:\n",
    "                loss +=  latent_zero_regularization_factor * latent_zero_regularization\n",
    "            if latent_variance_regularization_factor:\n",
    "                loss += -latent_variance_regularization_factor * latent_var_regularization\n",
    "            if latent_distance_regularization_factor:\n",
    "                loss +=  latent_distance_regularization_factor * latent_distance_regularization\n",
    "\n",
    "            for i, factor in enumerate(config.parameters_regularization_factor):\n",
    "                if factor:\n",
    "                    loss += factor * parameters_regularization[i]\n",
    "\n",
    "            data['loss'] += loss / images.shape[0]\n",
    "            data['log_prob'] += log_prob / images.shape[0]\n",
    "            data['decoder_distance'] += decoder_distance / images.shape[0]\n",
    "            data['latent_regularization/correlation'] += latent_cor_regularization / images.shape[0]\n",
    "            data['latent_regularization/zero'] += latent_zero_regularization / images.shape[0]\n",
    "            data['latent_regularization/variance'] += latent_var_regularization / images.shape[0]\n",
    "            data['latent_regularization/distance'] += latent_distance_regularization / images.shape[0]\n",
    "            for i, reg in enumerate(parameters_regularization):\n",
    "                data['parameters_regularization'][i] += reg / num_masks\n",
    "\n",
    "            if optimize:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if log_interval and (batch_idx + 1) % log_interval == 0:\n",
    "                print(\n",
    "                    '\\t{}\\t{:3d}/{:3d} - loss : {:.4f}, time : {:.3f}s'.format(\n",
    "                        loop_name, batch_idx, len(data_loader), data['loss'] / (1 + batch_idx), time.time() - time_)\n",
    "                )\n",
    "                time_ = time.time()\n",
    "    for key in data:\n",
    "        if isinstance(data[key], list):\n",
    "            for i in range(len(data[key])):\n",
    "                data[key][i] /= len(data_loader)\n",
    "        else:\n",
    "            data[key] /= len(data_loader)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loop(data_loader, record_input_images=False, record_reconstructions=False, loop_name='evaluation'):\n",
    "    with t.no_grad():\n",
    "        scores = t.Tensor().to(device)\n",
    "        reconstructed_images = t.Tensor().to(device)\n",
    "        features = t.Tensor().to(device)\n",
    "        labels = np.empty(0, dtype=np.int8)\n",
    "        input_images = t.Tensor().to(device)\n",
    "        time_ = time.time()\n",
    "        for batch_idx, (images, label) in enumerate(data_loader):\n",
    "            images = images.to(device)\n",
    "            if record_input_images:\n",
    "                input_images = t.cat((input_images, images), dim=0)\n",
    "\n",
    "            output, latent, reconstruction = model(images)\n",
    "            scores = t.cat((scores, model.made.log_prob_hitmap(latent, output).sum(1)), dim=0)\n",
    "            features = t.cat((features, latent), dim=0)\n",
    "            if record_reconstructions:\n",
    "                reconstructed_images = t.cat((reconstructed_images, reconstruction), dim=0)\n",
    "            labels = np.append(labels, label.numpy().astype(np.int8), axis=0)\n",
    "            if config.log_evaluation_loop_interval and (batch_idx + 1) % config.log_evaluation_loop_interval == 0:\n",
    "                print(\n",
    "                    '\\t{}\\t{:3d}/{:3d} - time : {:.3f}s'.format(\n",
    "                        loop_name, batch_idx, len(data_loader), time.time() - time_)\n",
    "                )\n",
    "                time_ = time.time()\n",
    "    return scores, features, labels, input_images, reconstructed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def submit_loop_data(data, title, epoch):\n",
    "    for key in data:\n",
    "        if isinstance(data[key], list):\n",
    "            for i in range(len(data[key])):\n",
    "                writer.add_scalar(f'{key}/{title}/{i}', data[key][i], epoch)\n",
    "        else:\n",
    "            writer.add_scalar(f'{key}/{title}', data[key], epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_network_weights(epoch):\n",
    "    for i, conv in enumerate(model.encoder.conv_layers):\n",
    "        writer.add_histogram(f'encoder/conv{i}', conv.weight, epoch)\n",
    "    for i, deconv in enumerate(model.decoder.deconv_layers):\n",
    "        writer.add_histogram(f'decoder/deconv{i}', deconv.weight, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_features(features, tag, epoch):\n",
    "    vector_size = features.shape[1]\n",
    "    for i in range(vector_size):\n",
    "        writer.add_histogram(f'{tag}/{i}', features[:, i], epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_extreme_cases(input_images, reconstructed_images, epoch, tag='worst_reconstructions',\n",
    "                         num_cases=config.num_extreme_cases):\n",
    "    distance_hitmap = model.decoder.distance_hitmap(input_images, reconstructed_images).detach().cpu().numpy()\n",
    "    distance = model.decoder.distance(input_images, reconstructed_images).detach().cpu().numpy()\n",
    "    sorted_indexes = np.argsort(distance)\n",
    "    result_images = np.empty((num_cases * 3, input_images.shape[1], input_images.shape[2], input_images.shape[3]))\n",
    "    for i, index in enumerate(sorted_indexes[:num_cases]):\n",
    "        result_images[i * 3] = input_images[index]\n",
    "        result_images[i * 3 + 1] = reconstructed_images[index]\n",
    "        result_images[i * 3 + 2] = distance_hitmap[index]\n",
    "    writer.add_images(tag, result_images, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    for epoch in range(config.max_epoch):\n",
    "        print('epoch {:4} - lr: {}'.format(epoch, optimizer.param_groups[0][\"lr\"]))\n",
    "        if config.validation_interval and epoch % config.validation_interval == 0:\n",
    "            validation_results = data_feed_loop(validation_loader, False, loop_name='validation')\n",
    "            submit_loop_data(validation_results, 'validation', epoch)\n",
    "\n",
    "        if config.evaluation_interval and epoch % config.evaluation_interval == 0:\n",
    "            record_extreme_cases = config.commit_images_interval and (epoch % (\n",
    "                    config.evaluation_interval * config.commit_images_interval) == 0)\n",
    "            scores, features, labels, images, reconstruction = evaluate_loop(\n",
    "                test_loader, record_extreme_cases, record_extreme_cases, loop_name='test evaluation')\n",
    "            writer.add_scalar(\n",
    "                f'auc{str(config.normal_classes)}',\n",
    "                roc_auc_score(y_true=np.isin(labels, config.normal_classes).astype(np.int8),\n",
    "                              y_score=scores.cpu()), epoch)\n",
    "            anomaly_indexes = (np.isin(labels, config.normal_classes) == False)\n",
    "            writer.add_histogram('log_probs/test/anomaly', scores[anomaly_indexes], epoch)\n",
    "            writer.add_histogram('log_probs/test/normal', scores[(anomaly_indexes == False)], epoch)\n",
    "            if record_extreme_cases:\n",
    "                submit_extreme_cases(\n",
    "                    images[anomaly_indexes], reconstruction[anomaly_indexes], epoch,\n",
    "                    tag='worst_reconstruction/test/anomaly')\n",
    "                submit_extreme_cases(\n",
    "                    images[(anomaly_indexes == False)], reconstruction[(anomaly_indexes == False)], epoch,\n",
    "                    tag='worst_reconstruction/test/normal')\n",
    "            submit_features(features[anomaly_indexes], 'features/test/anomaly', epoch)\n",
    "            submit_features(features[(anomaly_indexes == False)], 'features/test/normal', epoch)\n",
    "            \n",
    "            scores, features, labels, images, reconstruction = evaluate_loop(\n",
    "                train_loader, record_extreme_cases, record_extreme_cases, loop_name='train evaluation')\n",
    "            if record_extreme_cases:\n",
    "                submit_extreme_cases(images, reconstruction, epoch, tag='worst_reconstruction/train')\n",
    "            writer.add_histogram('log_probs/train', scores, epoch)\n",
    "            submit_features(features, 'features/train', epoch)\n",
    "            writer.flush()\n",
    "\n",
    "        if config.embedding_interval and (epoch + 1) % config.embedding_interval == 0:\n",
    "            scores, features, labels, images, reconstruction = evaluate_loop(test_loader, record_input_images=True,\n",
    "                                                                             loop_name='embedding process')\n",
    "            writer.add_embedding(features, metadata=labels, label_img=images, global_step=epoch,\n",
    "                                 tag=f'{model_name}/test')\n",
    "            writer.flush()\n",
    "\n",
    "        if config.track_weights_interval and epoch % config.track_weights_interval == 0:\n",
    "            submit_network_weights(epoch)\n",
    "\n",
    "        train_results = data_feed_loop(train_loader, True, loop_name='train')\n",
    "        submit_loop_data(train_results, 'train', epoch)\n",
    "\n",
    "        writer.add_scalar('learning_rate', optimizer.param_groups[0][\"lr\"], epoch)\n",
    "        scheduler.step()\n",
    "\n",
    "        if config.save_interval and (epoch + 1) % config.save_interval == 0:\n",
    "            model.save(config.models_dir + f'/{model_name}/{model_name}-E{epoch}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir=\"{}\" --host 0.0.0.0 --port 6006 &'.format(config.runs_dir)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().system_raw('../ngrok http 6006 &')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir f\"{config.runs_dir}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
